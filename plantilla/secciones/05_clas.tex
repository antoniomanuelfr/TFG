\chapter{Clasificación a partir de regresión}
Se ha comprobado que usando regresión, se ha conseguido unos buenos modelos para predecir un valor \textbf{preciso} de la intención emprendedora media. El siguiente paso realizado ha sido el de \textbf{categorizar} el caso de regresión, obteniendo así clases del tipo \textit{intención emprendedora alta, baja o media}.\\
Hay que recordar que el resultado obtenido por el modelo de aprendizaje puede ser leído por personal no familiarizado con matemáticas o informática. Este enfoque lo que nos permite es usar una única variable como objetivo con los valores \textit{alto, bajo o alto}, siendo así más legible la salida del algoritmo.\\
\linebreak
Para categorizar los valores de predicción se ha establecido unos rangos y se van a transformar los valores de ese rango en las clases alta. media. y baja. Para establecer el rango, se ha ejecutado varias veces los algoritmos seleccionados y se han comparado las métricas obtenidas, usando unos rangos para establecer estas clases.\\
Los rangos que se han probado son:
\begin{itemize}
	\item $(4, 6, 7)$
	\item $(3.5, 5.5, 7)$
	\item $(3, 5, 7)$
\end{itemize}
Estas tuplas representan el valor usado para determinar el valor límite para clasificar una muestra como emprendimiento bajo, medio o alto (en ese orden).\\
\linebreak
\section{Rendimiento de modelos de clasificación}
Al igual que en los modelos de regresión, en clasificación se hace uso de métricas para ver el comportamiento de los modelos entrenados. A continuación se explica qué métricas se han usado:
\subsection{Accuracy}
Esta métrica mide el porcentaje de casos que el modelo ha clasificado correctamente.
\[
	accuracy(y,\hat{y})=\frac{1}{n_{samples}}\sum_{i=1}^{n_{samples}}1(\hat{y_i}=y_i)
\]

\subsection{Matriz de confusión y métricas}
Cada entrada $i.j$ de la matriz se define como el número de observaciones del grupo $i$ que han sido predichas como del grupo $j$. Un ejemplo de matriz de confusión para el problema que se esta tratando es:
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.8]{conf_matrix.png}
	\caption{Ejemplo de matriz de confusión}
	\label{fig:conf_matrix}
\end{figure}
Aquí podemos ver cuantas muestras de una clase el modelo predijo como correctas (de la misma clase) y cuantas muestras de una clase predijo como erróneas (de distinta clase).\\
\linebreak
A partir de la matriz de confusión, se pueden sacar las siguiente métricas:
\[Precision = \frac{TP} {TP + FP}\]
\[Recall = \frac{TP}{TP + FN}\]
\[FPR = \frac{FP}{FP + TN}\]
\[ F1\_score = 2 \times \frac{Precision \times Recall}{Precision + Recall} \]
Intuitivamente, \textbf{Precision} mide la habilidad del modelo de no clasificar como \textbf{positiva} una muestra que es \textbf{negativa}, mientras que \textbf{Recall} mide la habilidad del modelo para clasificar bien todas las muestras positivas.\\
\linebreak
Cabe destacar que se han explicado para el caso de clasificación binaria, pero estas métricas se pueden extender para la clasificación multi-clase y multi-etiqueta.
\subsection{Curva ROC y AUC}
Partiendo de lo explicado sobre la matriz de confusión, la curva ROC (Receiver Operating Characteristic Curve) es un gráfico donde se representa el \textbf{FPR} (False Positive Rate) en el eje \textbf{X} y el \textbf{TPR} (True Positive Rate o Recall) en el eje \textbf{Y}. Se escogen estas métricas debido a la siguiente hipótesis:\\
A medida que incrementamos el ratio de verdaderos positivos, se va a incrementar el ratio de falsos positivos debido a que es más probable que el modelo clasifique como positivo una muestra que no es positiva.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{roc}
	\caption{Ejemplo de curva ROC}
	\label{fig:roc}
\end{figure}
Lo ideal, es que la figura se acerque a la esquina superior izquierda lo máximo posible, ya que implicaría que se esta clasificando como correctos todas las muestras positivas y ninguna muestra negativa se está clasificando como positiva.\\
Se puede usar el área bajo la curva como una métrica para comprobar como de bueno es el modelo. Esta métrica se conoce como \textbf{AUC} (Area Under Curve).\\
\linebreak
Al igual que en regresión (\ref{sec:validation}-\nameref{sec:validation}), se ha usado la técnica de \textit{k-fold validation} para validar el rendimiento de los modelos entrenados.
\section{Modelos de clasificación}
Para clasificación, se van a usar unicamente aquellos modelos que se ha demostrado empíricamente en la sección \ref{sec:algoritmos}-\nameref{sec:algoritmos} que han tenido un buen desempeño. Por tanto se van a usar Árboles de Decisión, Random Forest y SVM.
\subsection{Árboles de Decisión}
En esta sección se va a exponer los resultados obtenidos en clasificación usando \textbf{Árboles de Decisión}.\\
La siguiente tabla expone los resultados obtenidos:
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.7]{src/dt_cmp_val_metrics}
	\caption{Comparación en conjunto de validación}
	\label{fig:dtre_class_val}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.7]{src/dt_cmp_test_metrics}
	\caption{Comparación en conjunto de test}
	\label{fig:dtre_class_testl}
\end{figure}
A continuación, las tablas con los resultados de las métricas y las matrices de confusión obtenidas por los Árboles de decisión usando la categorización usando los rangos $(3.5,5.5,7)$ y $(4,6,7)$
\subsubsection*{Rango $(3.5,5.5,7)$}
\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|c|c|c}
		\cline{1-4}
		FOLD   & F1 Score & AUC Score & Accuracy \\ \cline{1-4}
		Fold 0 & 0.684    & 0.822     & 0.713    \\ \cline{1-4}
		Fold 1 & 0.699    & 0.881     & 0.725    \\ \cline{1-4}
		Fold 2 & 0.691    & 0.84      & 0.713    \\ \cline{1-4}
		Fold 3 & 0.664    & 0.833     & 0.701    \\ \cline{1-4}
		Fold 4 & 0.753    & 0.869     & 0.768    \\ \cline{1-4}
		Fold 5 & 0.698    & 0.849     & 0.724    \\ \cline{1-4}
		Train  & 0.781    & 0.909     & 0.794    \\ \cline{1-4}
		Test   & 0.701    & 0.843     & 0.715    \\ \cline{1-4}
	\end{tabular}
	\caption{Valores de métricas obtenidos usando rango $(3.5,5.5,7)$}
\end{table}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{src/confusion_matrix_dtree_classification_3-5_5-5_7.png}
	\caption{Matriz de confusión para Árboles de Decisión usando $(3.5,5.5,7)$}
	\label{fig:confusion_matrix_dtree1}
\end{figure}
\subsubsection*{Rango $(4,6,7)$}
\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|c|c|c}
		\cline{1-4}
		FOLD   & F1 Score & AUC Score & Accuracy \\ \cline{1-4}
		Fold 0 & 0.709    & 0.861     & 0.705    \\ \cline{1-4}
		Fold 1 & 0.686    & 0.828     & 0.681    \\ \cline{1-4}
		Fold 2 & 0.698    & 0.842     & 0.697    \\ \cline{1-4}
		Fold 3 & 0.715    & 0.855     & 0.709    \\ \cline{1-4}
		Fold 4 & 0.69     & 0.836     & 0.688    \\ \cline{1-4}
		Fold 5 & 0.7      & 0.844     & 0.696    \\ \cline{1-4}
		Train  & 0.752    & 0.89      & 0.746    \\ \cline{1-4}
		Test   & 0.721    & 0.867     & 0.72     \\ \cline{1-4}
	\end{tabular}
	\caption{Valores de métricas obtenidos usando rango $(4,6,7)$}
\end{table}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{src/confusion_matrix_dtree_classification_4_6_7}
	\caption{Matriz de confusión para Árboles de Decisión usando $(4,6,7)$}
	\label{fig:confusion_matrix_dtree2}
\end{figure}
\pagebreak
\subsection{Random Forest}
En esta sección se va a exponer los resultados obtenidos en clasificación usando \textbf{Random Forest}.
A continuación se muestran una serie de gráficos comparando las distintas métricas que se han seleccionado:
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{src/rf_class_cmp_val_metrics}
	\caption{Comparación en conjunto de validación}
	\label{fig:rf_class_cmp_val}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{src/rf_class_cmp_test_metrics}
	\caption{Comparación en conjunto de test}
	\label{fig:rf_class_cmp_test}
\end{figure}
En las secciones siguientes, se va a mostrar las tablas con los valores de métricas obtenidas por Random Forest y las matrices de confusión correspondientes.
\subsubsection*{Rango $(3,5,7)$}
\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|c|c|c}
		\cline{1-4}
		FOLD   & F1 Score & AUC Score & Accuracy \\ \cline{1-4}
		Fold 0 & 0.694    & 0.858     & 0.721    \\ \cline{1-4}
		Fold 1 & 0.755    & 0.898     & 0.769    \\ \cline{1-4}
		Fold 2 & 0.699    & 0.883     & 0.717    \\ \cline{1-4}
		Fold 3 & 0.693    & 0.867     & 0.709    \\ \cline{1-4}
		Fold 4 & 0.748    & 0.892     & 0.776    \\ \cline{1-4}
		Fold 5 & 0.718    & 0.88      & 0.738    \\ \cline{1-4}
		Train  & 0.993    & 1.0       & 0.994    \\\cline{1-4}
		Test   & 0.739    & 0.885     & 0.761    \\ \cline{1-4}
	\end{tabular}
	\caption{Valores de métricas obtenidos usando rango $(3,5,7)$}
\end{table}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{src/confusion_matrix_rf_class_3_5_7.png}
	\caption{Matriz de confusión para Random Forest usando $(3,5,7)$}
	\label{fig:confusion_matrix_rf1}
\end{figure}

\subsection*{Rango $(4,6,7)$}
\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|c|c|c}
		\cline{1-4}
		FOLD   & F1 Score & AUC Score & Accuracy \\ \cline{1-4}
		Fold 0 & 0.702    & 0.887     & 0.701    \\  \cline{1-4}
		Fold 1 & 0.72     & 0.884     & 0.713    \\  \cline{1-4}
		Fold 2 & 0.763    & 0.903     & 0.765    \\  \cline{1-4}
		Fold 3 & 0.747    & 0.891     & 0.745    \\  \cline{1-4}
		Fold 4 & 0.703    & 0.868     & 0.696    \\  \cline{1-4}
		Fold 5 & 0.727    & 0.887     & 0.724    \\  \cline{1-4}
		Train  & 0.998    & 1.0       & 0.998    \\ \cline{1-4}
		Test   & 0.748    & 0.901     & 0.746    \\ \cline{1-4}
	\end{tabular}
	\caption{Valores de métricas obtenidos usando rango $(4,6,7)$}
\end{table}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{src/confusion_matrix_rf_class_4_6_7.png}
	\caption{Matriz de confusión para Random Forest usando $(4,6,7)$}
	\label{fig:confusion_matrix_rf2}
\end{figure}
\pagebreak
\subsection{Support Vector Machines}
En esta sección se va a exponer los resultados obtenidos en clasificación usando \textbf{SVM}.
A continuación se muestran una serie de gráficos comparando las distintas métricas que se han seleccionado:
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{src/svc_cmp_val_metrics.png}
	\caption{Comparación en conjunto de validación}
	\label{fig:svc_class_cmp_val}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{src/svc_cmp_test_metrics.png}
	\caption{Comparación en conjunto de test}
	\label{fig:svc_class_cmp_test}
\end{figure}
En las secciones siguientes, se va a mostrar las tablas con los valores de métricas obtenidas por Random Forest y las matrices de confusión correspondientes.
\subsubsection*{Rango $(3.5,5.5,7)$}
\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|c|c|c}
		\cline{1-4}
		FOLD   & F1 Score & AUC Score & Accuracy \\ \cline{1-4}
		Fold 0 & 0.682    & 0.846     & 0.717    \\ \cline{1-4}
		Fold 1 & 0.713    & 0.887     & 0.753    \\ \cline{1-4}
		Fold 2 & 0.729    & 0.899     & 0.749    \\ \cline{1-4}
		Fold 3 & 0.698    & 0.861     & 0.713    \\ \cline{1-4}
		Fold 4 & 0.735    & 0.91      & 0.772    \\ \cline{1-4}
		Fold 5 & 0.711    & 0.881     & 0.741    \\ \cline{1-4}
		Train  & 0.928    & 0.989     & 0.933    \\ \cline{1-4}
		Test   & 0.712    & 0.887     & 0.742    \\ \cline{1-4}
	\end{tabular}
	\caption{Valores de métricas obtenidos usando rango $(3.5,5.5,7)$}
\end{table}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{src/confusion_matrix_svc_3-5_5-5_7.png}
	\caption{ confusión para SVM usando $(3.5,5.5,7)$}
	\label{fig:confusion_matrix_svm1}
\end{figure}

\subsubsection*{Rango $(4,6,7)$}
\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|c|c|c}
		\cline{1-4}
		FOLD   & F1 Score & AUC Score & Accuracy \\ \cline{1-4}
		Fold 0 & 0.65     & 0.856     & 0.649    \\  \cline{1-4}
		Fold 1 & 0.702    & 0.886     & 0.705    \\  \cline{1-4}
		Fold 2 & 0.742    & 0.88      & 0.745    \\  \cline{1-4}
		Fold 3 & 0.739    & 0.888     & 0.741    \\  \cline{1-4}
		Fold 4 & 0.681    & 0.862     & 0.676    \\  \cline{1-4}
		Fold 5 & 0.703    & 0.874     & 0.703    \\  \cline{1-4}
		Train  & 0.918    & 0.991     & 0.917    \\  \cline{1-4}
		Test   & 0.714    & 0.881     & 0.718    \\  \cline{1-4}
	\end{tabular}
	\caption{Valores de métricas obtenidos usando rango $(4,6,7)$}
\end{table}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{src/confusion_matrix_svc_4_6_7.png}
	\caption{Matriz de confusión para SVM usando $(4,6,7)$}
	\label{fig:confusion_matrix_svc2}
\end{figure}

\subsection{Conclusiones}
Como se puede apreciar, ninguno de los algoritmos no han tenido problemas clasificando la clase \textbf{IEAlta}, clasificando correctamente más del $80\%$ de las instancias de esta clase.
Los algoritmos han tenido más problemas a la hora de clasificar correctamente la clase \textbf{IEBaja}, clasificando correctamente solo el $50\%$. Volviendo a la figura \ref{tab:ocurrencia_valores} donde se mostraba el conteo de clases, se puede apreciar que la clase formada por los valores más bajos tiene una menor cantidad de muestras, explicando así que los algoritmos fallen más comúnmente en estas clases, clasificando una gran parte como \textbf{IEMedia} (tiene sentido ya que es la clase más 'cercana') .
Por
